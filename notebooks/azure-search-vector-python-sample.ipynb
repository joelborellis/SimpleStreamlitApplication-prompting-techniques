{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Cognitive Search Vector Search Code Sample with Azure OpenAI\n",
    "This code demonstrates how to use Azure Cognitive Search with OpenAI and Azure Python SDK\n",
    "## Prerequisites\n",
    "To run the code, install the following packages. Please use the latest pre-release version `pip install azure-search-documents --pre`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install azure-search-documents --pre\n",
    "! pip install openai\n",
    "! pip install python-dotenv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Vector' from 'azure.search.documents.models' (e:\\Projects\\Python\\lightfoot-streamlit\\.venv\\lib\\site-packages\\azure\\search\\documents\\models\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32me:\\Projects\\Python\\lightfoot-streamlit\\notebooks\\azure-search-vector-python-sample.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Projects/Python/lightfoot-streamlit/notebooks/azure-search-vector-python-sample.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mazure\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msearch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdocuments\u001b[39;00m \u001b[39mimport\u001b[39;00m SearchClient  \n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Projects/Python/lightfoot-streamlit/notebooks/azure-search-vector-python-sample.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mazure\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msearch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdocuments\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mindexes\u001b[39;00m \u001b[39mimport\u001b[39;00m SearchIndexClient  \n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Projects/Python/lightfoot-streamlit/notebooks/azure-search-vector-python-sample.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mazure\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msearch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdocuments\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Vector\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/Python/lightfoot-streamlit/notebooks/azure-search-vector-python-sample.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mazure\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msearch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdocuments\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mindexes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m (  \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/Python/lightfoot-streamlit/notebooks/azure-search-vector-python-sample.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     SearchIndex,  \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/Python/lightfoot-streamlit/notebooks/azure-search-vector-python-sample.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     SearchField,  \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/Python/lightfoot-streamlit/notebooks/azure-search-vector-python-sample.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     HnswVectorSearchAlgorithmConfiguration,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/Python/lightfoot-streamlit/notebooks/azure-search-vector-python-sample.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m ) \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/Python/lightfoot-streamlit/notebooks/azure-search-vector-python-sample.ipynb#W3sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Configure environment variables  \u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Vector' from 'azure.search.documents.models' (e:\\Projects\\Python\\lightfoot-streamlit\\.venv\\lib\\site-packages\\azure\\search\\documents\\models\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Import required libraries  \n",
    "import os  \n",
    "import json  \n",
    "import openai  \n",
    "from dotenv import load_dotenv  \n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt  \n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.indexes import SearchIndexClient  \n",
    "from azure.search.documents.models import Vector\n",
    "from azure.search.documents.indexes.models import (  \n",
    "    SearchIndex,  \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SimpleField,  \n",
    "    SearchableField,  \n",
    "    SearchIndex,  \n",
    "    SemanticConfiguration,  \n",
    "    PrioritizedFields,  \n",
    "    SemanticField,  \n",
    "    SearchField,  \n",
    "    SemanticSettings,  \n",
    "    VectorSearch,  \n",
    "    HnswVectorSearchAlgorithmConfiguration,\n",
    ") \n",
    "\n",
    "  \n",
    "# Configure environment variables  \n",
    "load_dotenv() \n",
    "model_deployment_name = os.getenv(\"AZURE_OPENAI_MODEL_NAME\")\n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\") \n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\") \n",
    "key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\") \n",
    "openai.api_type = \"azure\"  \n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")  \n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  \n",
    "openai.api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")  \n",
    "credential = AzureKeyCredential(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes unwanted characters from the txt file\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "# Path to the directory containing the text files\n",
    "directory = \"../web-crawler/txt/www.lightfoot.ca/\"\n",
    "\n",
    "# Iterate through the directory\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if the file is a text file\n",
    "    if filename.endswith(\".txt\"):\n",
    "        # Open the file in read mode and read its contents\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as f:\n",
    "            contents = f.read()\n",
    "        # Convert the encoding to utf-8\n",
    "        contents = contents.replace('\\n\\n', ' ').replace('\\n', ' ').replace('â€', '').replace('Â', '').replace('©', '').replace('*', '').replace('•', '').replace('*', '').replace('“', '').replace('”', '').replace('  ', ' ').replace(\"LYRICS/CHORDS\", '').replace('    ', ' ')\n",
    "        #contents = contents.encode('utf-8')\n",
    "        # Open the file in write mode and write the utf-8 encoded contents\n",
    "        with codecs.open(os.path.join(directory, filename), 'w', encoding='UTF-8') as f:\n",
    "            f.write(contents)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "\n",
    "# Function to generate embeddings for title and content fields, also used for query embeddings\n",
    "def generate_embeddings(text):\n",
    "    response = openai.Embedding.create(\n",
    "        input=text, engine=\"text-embedding-ada-002\")\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings\n",
    "\n",
    "# get a UUID - URL safe, Base64\n",
    "def get_a_uuid():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "# method to get the token length with the encoding\n",
    "tokenizer_name = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokenizer = tiktoken.get_encoding(tokenizer_name.name)\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(text, disallowed_special=())\n",
    "    return len(tokens)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=4000, # this depends on which model you might use, for example with the 16k GPT models setting this to 8k is reasonable and maybe higher\n",
    "    chunk_overlap=100,\n",
    "    length_function=tiktoken_len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "#function to return the number of tokens in a string\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    token_integers = encoding.encode(string)\n",
    "    num_tokens = len(token_integers)\n",
    "\n",
    "    return num_tokens\n",
    "\n",
    "def get_text_before_double_space(input_string):  \n",
    "    return input_string.split(\"  \", 1)[0] \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe with embeddings from txt files in a directory\n",
    "Read your txt files into a dataframe that can be used to load the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and read all the txt files and put them into chuncks in a dataframe, this takes the contents of\n",
    "# the file and splits based on the text splitter.  this needs to be split because of the embeddings\n",
    "# columns will be title, tokens, content\n",
    "\n",
    "directory = \"../web-crawler/txt/www.lightfoot.ca/\"\n",
    "chunk = {}\n",
    "txt = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(directory, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "                texts = text_splitter.create_documents([text])\n",
    "                for i in texts:\n",
    "                        chunk = {\n",
    "                                \"id\": get_a_uuid(),  # generate a random uuid for the document\n",
    "                                \"title\": get_text_before_double_space(i.page_content),\n",
    "                                #\"title\": filename[:-4],  # remove the .txt extension from the filename and use this as the title\n",
    "                                \"content\": i.page_content,\n",
    "                                \"sourcefile\": filename,\n",
    "                                \"content_tokens\": num_tokens_from_string(i.page_content, \"cl100k_base\"),\n",
    "                                \"category\": \"Lightfoot\",\n",
    "                                \"contentVector\": generate_embeddings(i.page_content)\n",
    "                                }\n",
    "                        txt.append(chunk)\n",
    "\n",
    "df = pd.DataFrame(txt)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write DataFrame to a csv file  \n",
    "df['title'].to_csv('output.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your search index\n",
    "Create your search index schema and vector search configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a search index\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint, credential=credential)\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),\n",
    "    SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"sourcefile\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"category\", type=SearchFieldDataType.String,\n",
    "                    filterable=True),\n",
    "    SearchField(name=\"contentVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=1536, vector_search_configuration=\"lightfoot-vector-config\"),\n",
    "]\n",
    "\n",
    "vector_search = VectorSearch(\n",
    "    algorithm_configurations=[\n",
    "        HnswVectorSearchAlgorithmConfiguration(\n",
    "            name=\"lightfoot-vector-config\",\n",
    "            kind=\"hnsw\",\n",
    "            parameters={\n",
    "                \"m\": 4,\n",
    "                \"efConstruction\": 400,\n",
    "                \"efSearch\": 500,\n",
    "                \"metric\": \"cosine\"\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"lightfoot-semantic-config\",\n",
    "    prioritized_fields=PrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "        prioritized_keywords_fields=[SemanticField(field_name=\"category\")],\n",
    "        prioritized_content_fields=[SemanticField(field_name=\"content\")],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields,\n",
    "                    vector_search=vector_search, semantic_settings=semantic_settings)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert text and embeddings into vector store from data frame\n",
    "Creates a list called sections from the dataframe to be loaded into the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate a list with the data we will use to store in the index\n",
    "import re\n",
    "\n",
    "def create_sections(df):\n",
    "    for index, row in df.iterrows():\n",
    "        yield {\n",
    "            \"id\": row[\"id\"],\n",
    "            \"title\": row[\"title\"],\n",
    "            \"content\": row[\"content\"],\n",
    "            \"sourcefile\": row[\"sourcefile\"],\n",
    "            \"category\": row[\"category\"],\n",
    "            \"contentVector\": row[\"contentVector\"],\n",
    "            \"@search.action\": \"upload\",\n",
    "        }\n",
    "        \n",
    "sections = create_sections(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the sections list into the index\n",
    "Loops thru a list called sections and creates a document ofr each item in the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_sections(sections):\n",
    "    print(\n",
    "        f\"Indexing sections into search index '{index_name}'\"\n",
    "    )\n",
    "\n",
    "    search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "    i = 0\n",
    "    batch = []\n",
    "    for s in sections:\n",
    "        batch.append(s)\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            results = search_client.upload_documents(documents=batch)\n",
    "            succeeded = sum([1 for r in results if r.succeeded])\n",
    "            print(f\"\\tIndexed {len(results)} sections, {succeeded} succeeded\")\n",
    "            batch = []\n",
    "\n",
    "    if len(batch) > 0:\n",
    "        results = search_client.upload_documents(documents=batch)\n",
    "        succeeded = sum([1 for r in results if r.succeeded])\n",
    "        print(f\"\\tIndexed {len(results)} sections, {succeeded} succeeded\")\n",
    "        \n",
    "index_sections(sections)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a vector similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pure Vector Search\n",
    "query = \"legend lives on from the chippewah\"  \n",
    "  \n",
    "search_client = SearchClient(service_endpoint, index_name, credential=credential)\n",
    "vector = Vector(value=generate_embeddings(query), k=1, fields=\"contentVector\")\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vectors= [vector],\n",
    "    select=[\"title\", \"content\", \"category\"],\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n",
    "    print(f\"Category: {result['category']}\\n\")  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Cross-Field Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Field Vector Search\n",
    "query = \"types of anxiety disorders\"  \n",
    "  \n",
    "search_client = SearchClient(service_endpoint, index_name, credential=credential)  \n",
    "vector = Vector(value=generate_embeddings(query), k=3, fields=\"contentVector, title\")  \n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vectors=[vector],\n",
    "    select=[\"title\", \"summary\", \"category\"],\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Summary: {result['summary']}\")  \n",
    "    print(f\"Category: {result['category']}\\n\")  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Multi-Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Vector Search\n",
    "query = \"challenger sales model\"  \n",
    "  \n",
    "search_client = SearchClient(service_endpoint, index_name, credential=credential)  \n",
    "vector1 = Vector(value=generate_embeddings(query), k=3, fields=\"summaryVector\")  \n",
    "vector2 = Vector(value=generate_embeddings(query), k=3, fields=\"contentVector\")  \n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vectors=[vector1, vector2],\n",
    "    select=[\"title\", \"content\", \"category\"],\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n",
    "    print(f\"Category: {result['category']}\\n\")  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Pure Vector Search with a filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pure Vector Search with Filter\n",
    "query = \"tools for software development\"  \n",
    "  \n",
    "search_client = SearchClient(service_endpoint, index_name, credential=credential)  \n",
    "vector = Vector(value=generate_embeddings(query), k=3, fields=\"contentVector\")  \n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vectors=[vector],\n",
    "    filter=\"category eq 'Challenger Customer'\",\n",
    "    select=[\"title\", \"content\", \"category\"]\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n",
    "    print(f\"Category: {result['category']}\\n\")  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Search\n",
    "query = \"dans records\"  \n",
    "  \n",
    "search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))  \n",
    "vector = Vector(value=generate_embeddings(query), k=3, fields=\"contentVector\")  \n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,  \n",
    "    vectors=[vector],\n",
    "    select=[\"title\", \"content\", \"category\"],\n",
    "    top=3\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n",
    "    print(f\"Category: {result['category']}\\n\")  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Semantic Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Hybrid Search\n",
    "query = \"types of anxiety disorders\"\n",
    "\n",
    "search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))\n",
    "vector = Vector(value=generate_embeddings(query), k=3, fields=\"summaryVector\")  \n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,  \n",
    "    vectors=[vector],\n",
    "    select=[\"title\", \"summary\", \"category\"],\n",
    "    query_type=\"semantic\", query_language=\"en-us\", semantic_configuration_name='dsm-semantic-config', query_caption=\"extractive\", query_answer=\"extractive\",\n",
    "    top=3\n",
    ")\n",
    "\n",
    "semantic_answers = results.get_answers()\n",
    "for answer in semantic_answers:\n",
    "    if answer.highlights:\n",
    "        print(f\"Semantic Answer: {answer.highlights}\")\n",
    "    else:\n",
    "        print(f\"Semantic Answer: {answer.text}\")\n",
    "    print(f\"Semantic Answer Score: {answer.score}\\n\")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Summary: {result['summary']}\")\n",
    "    print(f\"Category: {result['category']}\")\n",
    "\n",
    "    captions = result[\"@search.captions\"]\n",
    "    if captions:\n",
    "        caption = captions[0]\n",
    "        if caption.highlights:\n",
    "            print(f\"Caption: {caption.highlights}\\n\")\n",
    "        else:\n",
    "            print(f\"Caption: {caption.text}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
