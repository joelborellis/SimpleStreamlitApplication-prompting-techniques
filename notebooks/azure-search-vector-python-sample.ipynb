{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Cognitive Search Vector Search Code Sample with Azure OpenAI\n",
    "This code demonstrates how to use Azure Cognitive Search with OpenAI and Azure Python SDK\n",
    "## Prerequisites\n",
    "To run the code, install the following packages. Please use the latest pre-release version `pip install azure-search-documents --pre`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install azure-search-documents --pre\n",
    "! pip install openai\n",
    "! pip install python-dotenv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries  \n",
    "import os  \n",
    "import json  \n",
    "import openai  \n",
    "from dotenv import load_dotenv  \n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt  \n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.indexes import SearchIndexClient  \n",
    "from azure.search.documents.models import Vector\n",
    "from azure.search.documents.indexes.models import (  \n",
    "    SearchIndex,  \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SimpleField,  \n",
    "    SearchableField,  \n",
    "    SearchIndex,  \n",
    "    SemanticConfiguration,  \n",
    "    PrioritizedFields,  \n",
    "    SemanticField,  \n",
    "    SearchField,  \n",
    "    SemanticSettings,  \n",
    "    VectorSearch,  \n",
    "    HnswVectorSearchAlgorithmConfiguration,\n",
    ") \n",
    "\n",
    "  \n",
    "# Configure environment variables  \n",
    "load_dotenv() \n",
    "model_deployment_name = os.getenv(\"AZURE_OPENAI_MODEL_NAME\")\n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\") \n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\") \n",
    "key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\") \n",
    "openai.api_type = \"azure\"  \n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")  \n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  \n",
    "openai.api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")  \n",
    "credential = AzureKeyCredential(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes unwanted characters from the txt file\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "# Path to the directory containing the text files\n",
    "directory = \"../web-crawler/txt/www.lightfoot.ca/\"\n",
    "\n",
    "# Iterate through the directory\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if the file is a text file\n",
    "    if filename.endswith(\".txt\"):\n",
    "        # Open the file in read mode and read its contents\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as f:\n",
    "            contents = f.read()\n",
    "        # Convert the encoding to utf-8\n",
    "        contents = contents.replace('\\n\\n', ' ').replace('\\n', ' ').replace('â€', '').replace('Â', '').replace('©', '').replace('*', '').replace('•', '').replace('*', '').replace('“', '').replace('”', '').replace('  ', ' ').replace(\"LYRICS/CHORDS\", '').replace('    ', ' ')\n",
    "        #contents = contents.encode('utf-8')\n",
    "        # Open the file in write mode and write the utf-8 encoded contents\n",
    "        with codecs.open(os.path.join(directory, filename), 'w', encoding='UTF-8') as f:\n",
    "            f.write(contents)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "\n",
    "# Function to generate embeddings for title and content fields, also used for query embeddings\n",
    "def generate_embeddings(text):\n",
    "    response = openai.Embedding.create(\n",
    "        input=text, engine=\"text-embedding-ada-002\")\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings\n",
    "\n",
    "# get a UUID - URL safe, Base64\n",
    "def get_a_uuid():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "# method to get the token length with the encoding\n",
    "tokenizer_name = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokenizer = tiktoken.get_encoding(tokenizer_name.name)\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(text, disallowed_special=())\n",
    "    return len(tokens)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=4000, # this depends on which model you might use, for example with the 16k GPT models setting this to 8k is reasonable and maybe higher\n",
    "    chunk_overlap=100,\n",
    "    length_function=tiktoken_len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "#function to return the number of tokens in a string\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    token_integers = encoding.encode(string)\n",
    "    num_tokens = len(token_integers)\n",
    "\n",
    "    return num_tokens\n",
    "\n",
    "def get_text_before_double_space(input_string):  \n",
    "    return input_string.split(\"  \", 1)[0] \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe with embeddings from txt files in a directory\n",
    "Read your txt files into a dataframe that can be used to load the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and read all the txt files and put them into chuncks in a dataframe, this takes the contents of\n",
    "# the file and splits based on the text splitter.  this needs to be split because of the embeddings\n",
    "# columns will be title, tokens, content\n",
    "\n",
    "directory = \"../web-crawler/txt/www.lightfoot.ca/\"\n",
    "chunk = {}\n",
    "txt = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(directory, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "                texts = text_splitter.create_documents([text])\n",
    "                for i in texts:\n",
    "                        chunk = {\n",
    "                                \"id\": get_a_uuid(),  # generate a random uuid for the document\n",
    "                                \"title\": get_text_before_double_space(i.page_content),\n",
    "                                #\"title\": filename[:-4],  # remove the .txt extension from the filename and use this as the title\n",
    "                                \"content\": i.page_content,\n",
    "                                \"sourcefile\": filename,\n",
    "                                \"content_tokens\": num_tokens_from_string(i.page_content, \"cl100k_base\"),\n",
    "                                \"category\": \"Lightfoot\",\n",
    "                                \"contentVector\": generate_embeddings(i.page_content)\n",
    "                                }\n",
    "                        txt.append(chunk)\n",
    "\n",
    "df = pd.DataFrame(txt)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write DataFrame to a csv file  \n",
    "df['title'].to_csv('output.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your search index\n",
    "Create your search index schema and vector search configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a search index\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint, credential=credential)\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),\n",
    "    SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"sourcefile\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"category\", type=SearchFieldDataType.String,\n",
    "                    filterable=True),\n",
    "    SearchField(name=\"contentVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=1536, vector_search_configuration=\"lightfoot-vector-config\"),\n",
    "]\n",
    "\n",
    "vector_search = VectorSearch(\n",
    "    algorithm_configurations=[\n",
    "        HnswVectorSearchAlgorithmConfiguration(\n",
    "            name=\"lightfoot-vector-config\",\n",
    "            kind=\"hnsw\",\n",
    "            parameters={\n",
    "                \"m\": 4,\n",
    "                \"efConstruction\": 400,\n",
    "                \"efSearch\": 500,\n",
    "                \"metric\": \"cosine\"\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"lightfoot-semantic-config\",\n",
    "    prioritized_fields=PrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "        prioritized_keywords_fields=[SemanticField(field_name=\"category\")],\n",
    "        prioritized_content_fields=[SemanticField(field_name=\"content\")],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields,\n",
    "                    vector_search=vector_search, semantic_settings=semantic_settings)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert text and embeddings into vector store from data frame\n",
    "Creates a list called sections from the dataframe to be loaded into the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate a list with the data we will use to store in the index\n",
    "import re\n",
    "\n",
    "def create_sections(df):\n",
    "    for index, row in df.iterrows():\n",
    "        yield {\n",
    "            \"id\": row[\"id\"],\n",
    "            \"title\": row[\"title\"],\n",
    "            \"content\": row[\"content\"],\n",
    "            \"sourcefile\": row[\"sourcefile\"],\n",
    "            \"category\": row[\"category\"],\n",
    "            \"contentVector\": row[\"contentVector\"],\n",
    "            \"@search.action\": \"upload\",\n",
    "        }\n",
    "        \n",
    "sections = create_sections(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the sections list into the index\n",
    "Loops thru a list called sections and creates a document ofr each item in the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_sections(sections):\n",
    "    print(\n",
    "        f\"Indexing sections into search index '{index_name}'\"\n",
    "    )\n",
    "\n",
    "    search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "    i = 0\n",
    "    batch = []\n",
    "    for s in sections:\n",
    "        batch.append(s)\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            results = search_client.upload_documents(documents=batch)\n",
    "            succeeded = sum([1 for r in results if r.succeeded])\n",
    "            print(f\"\\tIndexed {len(results)} sections, {succeeded} succeeded\")\n",
    "            batch = []\n",
    "\n",
    "    if len(batch) > 0:\n",
    "        results = search_client.upload_documents(documents=batch)\n",
    "        succeeded = sum([1 for r in results if r.succeeded])\n",
    "        print(f\"\\tIndexed {len(results)} sections, {succeeded} succeeded\")\n",
    "        \n",
    "index_sections(sections)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a vector similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Passing Ship\n",
      "Score: 0.8275232\n",
      "Content: A Passing Ship  A Passing Ship       A passing ship, I have found the open ocean Give me no lip, the waves roll by as I press on A sunlit sea, on the first day in April How fresh the wind, will you miss me when I'm gone How many words, how many songs still unwritten How many ships of the line have come and gone In the good old days, may they never be forgotten They had heavy wind or they had no wind at all         A passing ship, it is midnight on the ocean Had a real long trip, I have been at sea all winter When my ship came in, I was giving up the ghost I think I should be, leaving those passing ships alone When the sea runs high, the sea runs wild and I'm unsteady And I think of you, in the warmth of your home and family When love is true, there is no truer occupation And may this gale, blow us to the ones we love          Another day, another ocean Give me no lip, but stand aside as I press on A sunlit sea, on the last day of October How fresh the wind, will you miss me from now on I guess I should be leaving passing ships alone\n",
      "Category: Lightfoot\n",
      "\n",
      "Title: Christian Island\n",
      "Score: 0.8224941\n",
      "Content: Christian Island  Christian Island  I'm sailing down the summer wind I got whiskers on my chin And I like the mood I'm in As I while away the time of day In the lee of Christian Island Tall and strong she dips and reels I call her Silver Heels And she tells me how she feels She's a good old boat and she'll stay afloat Through the toughest gales and keep smiling But for one more day she would like to stay In the lee of Christian Island I'm sailing down the summer day Where fish and seagulls play I put my troubles all away And when the gale comes up I'll fill my cup With the whiskey of the highlands She's a good old ship and she'll make the trip From the lee of Christian Island Tall and strong she slips along I sing for her a song And she leans into the wind She's a good old boat and she'll stay afloat Through the toughest gales and keep smilin' When the summer ends we will rest again In the lee of Christian Island When the summer ends we will rest again In the lee of Christian Island\n",
      "Category: Lightfoot\n",
      "\n",
      "Title: Ballad Of The Yarmouth Castle\n",
      "Score: 0.8203453\n",
      "Content: Ballad Of The Yarmouth Castle  Ballad Of The Yarmouth Castle Well, it's four o'clock in the afternoon The anchors have been weighed From Miami to Nassau She's bound across the waves She's heading south through Biscayne Bay Into the open sea Yarmouth Castle, she's a-dying and don't know it Now the many years she's been to sea She's seen the better times She gives a groan of protest As they cast away her lines And the grumble of her engines And the rust along her spine Tells the Castle she's too old to be sailing But the sands run out within her heart A tiny spark glows red It smoulders through the evening There's laughter overhead Now the dinner's served and the cards are dealt And the drinks are passed around Deep within the fire starts a-burning Now it's midnight on the open sea And the moon is shining bright Some people join the party And others say goodnight There's many who are sleeping now It's been a busy day And a tiny wisp of smoke is a-rising \"Oh Lord,\" she groans, \"I'm burning!\" \"Let someone understand\" But her silent plea is wasted In the playing of the band Everybody's dancing on the deck And they're having such a time Then a voice says \"Shut up and deal, I'm losing\" Deep within the Yarmouth Castle The fire begins to glow It leaps into the hallways And climbs and twists and grows And the paint she wore to keep her young Oh Lord, how well it burns And soon that old fire is a-raging Well beneath the bridge it's climbing fast The captain stands aloft He calls up to the bosin, says \"Bosin, we are lost\" For the ragged hoses in the racks No pressure do they hold And the people down below will soon be dying All amidships, oh she's blazing now It's spreading fore and aft The people are a-scrambling As the fire blocks their path The evil smoke surrounds them And they're falling in their tracks And the captain in his lifeboat is a-leaving   Oh then the ship, Bahama Star Comes steaming through the night She sees the Castle blazing And 'tis a terrible sight \"Jump down, jump down\" the captain cries \"We'll save you if we can\" Then the paint on his funnels is a-frying \"God help the ones who sleep below And cannot find their way Thank God for those we've rescued Upon this awful day\" Now the heroes, they are many But the times are growing slim And now from stern to bow she's a-blazing Oh the Yarmouth Castle's moaning She's crying like a child You can hear her if you listen Above the roar so wild Is she crying for the ones who lie Within her molten sides Or crying for herself, I'm a-wondering But the living soon were rescued The ones who lived to tell And from the Star they watched her As she died there in the swells Like a toy ship on a millpond She burned all through the night Then slipped beneath the waves in the morning\n",
      "Category: Lightfoot\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pure Vector Search\n",
    "query = \"ship in storm\"  \n",
    "  \n",
    "search_client = SearchClient(service_endpoint, index_name, credential=credential)\n",
    "vector = Vector(value=generate_embeddings(query), k=3, fields=\"contentVector\")\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vectors= [vector],\n",
    "    select=[\"title\", \"content\", \"category\"],\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n",
    "    print(f\"Category: {result['category']}\\n\")  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Cross-Field Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Field Vector Search\n",
    "query = \"types of anxiety disorders\"  \n",
    "  \n",
    "search_client = SearchClient(service_endpoint, index_name, credential=credential)  \n",
    "vector = Vector(value=generate_embeddings(query), k=3, fields=\"contentVector, title\")  \n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vectors=[vector],\n",
    "    select=[\"title\", \"summary\", \"category\"],\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Summary: {result['summary']}\")  \n",
    "    print(f\"Category: {result['category']}\\n\")  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Multi-Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Vector Search\n",
    "query = \"challenger sales model\"  \n",
    "  \n",
    "search_client = SearchClient(service_endpoint, index_name, credential=credential)  \n",
    "vector1 = Vector(value=generate_embeddings(query), k=3, fields=\"summaryVector\")  \n",
    "vector2 = Vector(value=generate_embeddings(query), k=3, fields=\"contentVector\")  \n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vectors=[vector1, vector2],\n",
    "    select=[\"title\", \"content\", \"category\"],\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n",
    "    print(f\"Category: {result['category']}\\n\")  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Pure Vector Search with a filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pure Vector Search with Filter\n",
    "query = \"tools for software development\"  \n",
    "  \n",
    "search_client = SearchClient(service_endpoint, index_name, credential=credential)  \n",
    "vector = Vector(value=generate_embeddings(query), k=3, fields=\"contentVector\")  \n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vectors=[vector],\n",
    "    filter=\"category eq 'Challenger Customer'\",\n",
    "    select=[\"title\", \"content\", \"category\"]\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n",
    "    print(f\"Category: {result['category']}\\n\")  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Search\n",
    "query = \"dans records\"  \n",
    "  \n",
    "search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))  \n",
    "vector = Vector(value=generate_embeddings(query), k=3, fields=\"contentVector\")  \n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,  \n",
    "    vectors=[vector],\n",
    "    select=[\"title\", \"content\", \"category\"],\n",
    "    top=3\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n",
    "    print(f\"Category: {result['category']}\\n\")  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Semantic Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Hybrid Search\n",
    "query = \"types of anxiety disorders\"\n",
    "\n",
    "search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))\n",
    "vector = Vector(value=generate_embeddings(query), k=3, fields=\"summaryVector\")  \n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,  \n",
    "    vectors=[vector],\n",
    "    select=[\"title\", \"summary\", \"category\"],\n",
    "    query_type=\"semantic\", query_language=\"en-us\", semantic_configuration_name='dsm-semantic-config', query_caption=\"extractive\", query_answer=\"extractive\",\n",
    "    top=3\n",
    ")\n",
    "\n",
    "semantic_answers = results.get_answers()\n",
    "for answer in semantic_answers:\n",
    "    if answer.highlights:\n",
    "        print(f\"Semantic Answer: {answer.highlights}\")\n",
    "    else:\n",
    "        print(f\"Semantic Answer: {answer.text}\")\n",
    "    print(f\"Semantic Answer Score: {answer.score}\\n\")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Summary: {result['summary']}\")\n",
    "    print(f\"Category: {result['category']}\")\n",
    "\n",
    "    captions = result[\"@search.captions\"]\n",
    "    if captions:\n",
    "        caption = captions[0]\n",
    "        if caption.highlights:\n",
    "            print(f\"Caption: {caption.highlights}\\n\")\n",
    "        else:\n",
    "            print(f\"Caption: {caption.text}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
