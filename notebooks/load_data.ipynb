{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries  \n",
    "import os  \n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv  \n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient \n",
    "from azure.search.documents.models import (\n",
    "    VectorizedQuery,\n",
    "    VectorFilterMode,    \n",
    "    QueryAnswerType,\n",
    "    QueryCaptionType,\n",
    "    QueryType,\n",
    ")\n",
    "  \n",
    "# Configure environment variables  \n",
    "load_dotenv()  \n",
    "\n",
    "#Azure Search setup\n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\") \n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\") \n",
    "key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\") \n",
    "credential = AzureKeyCredential(key)\n",
    "\n",
    "#Azure openAI setup\n",
    "model: str = \"text-embedding-ada-002\"\n",
    "openai_client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2023-12-01-preview\",\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "openai_model = os.getenv(\"AZURE_OPENAI_MODEL_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import uuid\n",
    "\n",
    "# method to get the token length with the encoding\n",
    "tokenizer_name = tiktoken.encoding_for_model(\"gpt-4-32k-0613\")\n",
    "tokenizer = tiktoken.get_encoding(tokenizer_name.name)\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(text, disallowed_special=())\n",
    "    return len(tokens)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=8000, # this depends on which model you might use, for example with the 16k GPT models setting this to 8k is reasonable and maybe higher\n",
    "    chunk_overlap=100,\n",
    "    length_function=tiktoken_len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "# get a UUID - URL safe, Base64\n",
    "def get_a_uuid():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "#function to return the number of tokens in a string\n",
    "def num_tokens_from_string(string: str, model_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    #encoding = tiktoken.get_encoding(encoding_name)\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    token_integers = encoding.encode(string)\n",
    "    num_tokens = len(token_integers)\n",
    "\n",
    "    return num_tokens\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return openai_client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "def open_file(filepath):\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as infile:\n",
    "            return infile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some file cleanup, this code removes the space at the beginning of the txt file and renames it to the title of the song\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "input_directory = \"../web-crawler/txt/www.lightfoot.ca/\"\n",
    "output_directory = \"../web-crawler/txt/www.lightfoot.ca/cleaned/\"\n",
    "\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(input_directory, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "\n",
    "                content = f.read()\n",
    "                if content[0] == ' ':  # check if the first character is a space\n",
    "                    content = content[1:]  # remove the first character space\n",
    "                \n",
    "                index = content.find(\"  \")  # find the index of the first occurrence of double space this is the title\n",
    "                        # check if double space exists in the content\n",
    "                if index != -1:\n",
    "                    # get the text before the double space\n",
    "                    text_before_double_space = content[:index]\n",
    "                    # wrap the text in brackets\n",
    "                    #text_before_double_space = '[' + text_before_double_space + ']'\n",
    "                    # replace the original text with the modified text in the content\n",
    "                    #print(text_before_double_space)\n",
    "                    #content = content[:index] + content[index:].replace(text_before_double_space, '')\n",
    "                    #print(content)\n",
    "\n",
    "                #content = ' '.join(content.split()) # replace multiple spaces with single space\n",
    "\n",
    "                # write the modified content back to the file\n",
    "                with open(os.path.join(output_directory, f\"{text_before_double_space}.txt\"), 'w', encoding=\"utf-8\") as file:\n",
    "                    file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some file cleanup, this code removes the xtra stuff before the occuance of the spacing\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "input_directory = \"../web-crawler/txt/www.lightfoot.ca/\"\n",
    "output_directory = \"../web-crawler/txt/www.lightfoot.ca/cleaned/\"\n",
    "\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(input_directory, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "                \n",
    "                content = f.read()\n",
    "                index = content.find(\"  \")  # find the index of the first occurrence of double space this is the title\n",
    "\n",
    "                # Remove everything before the first occurrence (including the occurrence itself)\n",
    "                result = content[index+2:]\n",
    "\n",
    "                # write the modified content back to the file\n",
    "                with open(os.path.join(output_directory, filename), 'w', encoding=\"utf-8\") as file:\n",
    "                    file.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some file cleanup, this code removes the space at the beginning of the file and all double spacing\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "input_directory = \"../web-crawler/txt/www.lightfoot.ca/\"\n",
    "output_directory = \"../web-crawler/txt/www.lightfoot.ca/cleaned/\"\n",
    "\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(input_directory, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "\n",
    "                content = f.read()\n",
    "                if content[0] == ' ':  # check if the first character is a space\n",
    "                    content = content[1:]  # remove the first character space\n",
    "\n",
    "                content = ' '.join(content.split()) # replace multiple spaces with single space\n",
    "\n",
    "                # write the modified content back to the file\n",
    "                with open(os.path.join(output_directory, filename), 'w', encoding=\"utf-8\") as file:\n",
    "                    file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "directory = \"../web-crawler/txt/www.lightfoot.ca/cleaned\"\n",
    "chunk = {}\n",
    "txt = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(directory, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "                texts = text_splitter.create_documents([text])\n",
    "                doc_count = 0\n",
    "                for i in texts:\n",
    "                    #words = i.page_content.split(\"  \")\n",
    "                    chunk = {\n",
    "                        \"id\": get_a_uuid(),  # generate a random uuid for the document\n",
    "                        #\"title\": f\"{filename[:-4]}_Part_{doc_count}\",  # remove the .txt extension from the filename and use this as the title\n",
    "                        \"title\": f\"{filename[:-4]}\",  # remove the .txt extension from the filename and use this as the title\n",
    "                        #\"title\": words[0],  # the title is the text before the first double space\n",
    "                        \"content\": i.page_content,\n",
    "                        \"sourcefile\": filename,\n",
    "                        \"content_tokens\": num_tokens_from_string(i.page_content, \"gpt-4-32k-0613\"),\n",
    "                        \"category\": \"Gordon Lightfoot\",\n",
    "                        \"contentVector\": get_embedding(i.page_content)\n",
    "                        }\n",
    "                    txt.append(chunk)\n",
    "                    doc_count += 1\n",
    "\n",
    "df = pd.DataFrame(txt)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate a list with the data we will use to store in the index\n",
    "def create_sections(df):\n",
    "    for index, row in df.iterrows():\n",
    "        yield {\n",
    "            \"id\": row[\"id\"],\n",
    "            \"title\": row[\"title\"],\n",
    "            \"content\": row[\"content\"],\n",
    "            \"sourcefile\": row[\"sourcefile\"],\n",
    "            \"category\": row[\"category\"],\n",
    "            \"contentVector\": row[\"contentVector\"],\n",
    "            \"@search.action\": \"upload\",\n",
    "        }\n",
    "        \n",
    "sections = create_sections(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_sections(sections):\n",
    "    print(\n",
    "        f\"Indexing sections into search index '{index_name}'\"\n",
    "    )\n",
    "\n",
    "    search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "    i = 0\n",
    "    batch = []\n",
    "    for s in sections:\n",
    "        batch.append(s)\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            results = search_client.upload_documents(documents=batch)\n",
    "            succeeded = sum([1 for r in results if r.succeeded])\n",
    "            print(f\"\\tIndexed {len(results)} sections, {succeeded} succeeded\")\n",
    "            batch = []\n",
    "\n",
    "    if len(batch) > 0:\n",
    "        results = search_client.upload_documents(documents=batch)\n",
    "        succeeded = sum([1 for r in results if r.succeeded])\n",
    "        print(f\"\\tIndexed {len(results)} sections, {succeeded} succeeded\")\n",
    "        \n",
    "index_sections(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pure Vector Search\n",
    "query = \"we have a panic situation\"  \n",
    "  \n",
    "search_client = SearchClient(service_endpoint, index_name, credential=credential)\n",
    "vector_query = VectorizedQuery(vector=get_embedding(query), k_nearest_neighbors=3, fields=\"contentVector\")\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"title\", \"sourcefile\", \"category\", \"content\"],\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Sourcefile: {result['sourcefile']}\")  \n",
    "    print(f\"Category: {result['category']}\\n\")  \n",
    "    print(f\"Content: {result['content']}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pure Vector Search\n",
    "query = \"we have a panic situation\"  \n",
    "  \n",
    "search_client = SearchClient(service_endpoint, index_name, credential=credential)\n",
    "vector_query = VectorizedQuery(vector=get_embedding(query), k_nearest_neighbors=3, fields=\"contentVector\", exhaustive=True)\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"title\", \"sourcefile\", \"content\", \"category\"],\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Sourcefile: {result['sourcefile']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n",
    "    print(f\"Category: {result['category']}\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid with category\n",
    "query = \"dont confess\"  \n",
    "  \n",
    "search_client = SearchClient(service_endpoint, index_name, credential=credential)\n",
    "vector_query = VectorizedQuery(vector=get_embedding(query), k_nearest_neighbors=3, fields=\"contentVector\")\n",
    "  \n",
    "category = \"Gordon Lightfoot\"\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,  \n",
    "    vector_queries= [vector_query],\n",
    "    vector_filter_mode=VectorFilterMode.PRE_FILTER,\n",
    "    filter=f\"category eq '{category}'\",\n",
    "    #filter=\"category eq 'Microsoft'\",\n",
    "    select=[\"title\", \"sourcefile\", \"content\", \"category\"],\n",
    ")\n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n",
    "    print(f\"Category: {result['category']}\\n\")  \n",
    "    print(f\"Sourcefile: {result['sourcefile']}\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Hybrid Search\n",
    "query = \"loss of loved ones in a tragic accident on water\"\n",
    "\n",
    "search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))\n",
    "vector_query = VectorizedQuery(vector=get_embedding(query), k_nearest_neighbors=3, fields=\"contentVector\")\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,  \n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"title\", \"content\", \"category\"],\n",
    "    query_type=QueryType.SEMANTIC, semantic_configuration_name='lyrics-semantic-config', query_caption=QueryCaptionType.EXTRACTIVE, query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "    top=3\n",
    ")\n",
    "\n",
    "semantic_answers = results.get_answers()\n",
    "for answer in semantic_answers:\n",
    "    if answer.highlights:\n",
    "        print(f\"Semantic Answer: {answer.highlights}\")\n",
    "    else:\n",
    "        print(f\"Semantic Answer: {answer.text}\")\n",
    "    print(f\"Semantic Answer Score: {answer.score}\\n\")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Reranker Score: {result['@search.reranker_score']}\")\n",
    "    print(f\"Content: {result['content']}\")\n",
    "    print(f\"Category: {result['category']}\")\n",
    "\n",
    "    captions = result[\"@search.captions\"]\n",
    "    if captions:\n",
    "        caption = captions[0]\n",
    "        if caption.highlights:\n",
    "            print(f\"Caption: {caption.highlights}\\n\")\n",
    "        else:\n",
    "            print(f\"Caption: {caption.text}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
